# Use Python 3.11 with slim base
FROM python:3.11-slim

# Install Java 11 (PySpark 3.4.1 requires Java 11)
RUN apt-get update && \
    mkdir -p /etc/apt/keyrings && \
    apt-get install -y wget gnupg && \
    wget -O - https://packages.adoptium.net/artifactory/api/gpg/key/public | tee /etc/apt/keyrings/adoptium.asc && \
    echo "deb [signed-by=/etc/apt/keyrings/adoptium.asc] https://packages.adoptium.net/artifactory/deb $(awk -F= '/^VERSION_CODENAME/{print$2}' /etc/os-release) main" | tee /etc/apt/sources.list.d/adoptium.list && \
    apt-get update && \
    apt-get install -y temurin-11-jre procps && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy minimal requirements file for Spark consumer
COPY requirements-spark.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements-spark.txt

# Copy application code
COPY Spark/consumer.py /app/consumer.py

# Copy model files for anomaly detection
COPY Models/Chaima/scaler.pkl /app/scaler.pkl
COPY Models/Chaima/numeric_features.json /app/selected_features.json
COPY Models/Chaima/lstm_autoencoder.h5 /app/lstm_autoencoder.h5
COPY Models/Chaima/threshold.json /app/threshold.json

# Set JAVA_HOME for Java 11
ENV JAVA_HOME=/usr/lib/jvm/temurin-11-jre-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Run the consumer
CMD ["python", "consumer.py"]
