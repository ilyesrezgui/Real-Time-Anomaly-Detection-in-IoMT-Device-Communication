{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17380,
     "status": "ok",
     "timestamp": 1763073115056,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "22Ii78L1HNsG",
    "outputId": "b7602e27-dd66-4256-bf19-5688738bd1a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# --- First cell for all notebooks: works locally and in Google Colab ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # BASE_PATH: repo folder name after cloning in Colab\n",
    "    BASE_PATH = \"/content/Real-Time-Anomaly-Detection-in-IoMT-AD-Project\"\n",
    "else:\n",
    "    # Local environment: BASE_PATH = project root (one level up from notebooks)\n",
    "    BASE_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "print(f\"ðŸ“ BASE_PATH detected as: {BASE_PATH}\")\n",
    "\n",
    "# Add Spark and Models folders to Python path for imports\n",
    "sys.path.append(os.path.join(BASE_PATH, \"Spark\"))\n",
    "sys.path.append(os.path.join(BASE_PATH, \"Models\"))\n",
    "print(\"âœ… PYTHONPATH updated. Spark and Models folders included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1763073670577,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "cXg7kiHPWHv_"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JmlarviZctO"
   },
   "source": [
    "# Load scaler + feature maps + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1763073674163,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "NnoVK6FTWSV4",
    "outputId": "6e41c28c-f30a-450b-c1e3-ab12ccc2cd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded OK!\n"
     ]
    }
   ],
   "source": [
    "# Load scaler\n",
    "scaler = joblib.load(os.path.join(BASE_PATH, \"Models/scaler.pkl\"))\n",
    "print(\"Scaler loaded âœ…\")\n",
    "\n",
    "# Load selected features\n",
    "with open(os.path.join(BASE_PATH, \"Spark/selected_features.json\")) as f:\n",
    "    selected_features = json.load(f)\n",
    "\n",
    "# Load all features\n",
    "with open(os.path.join(BASE_PATH, \"Dataset/intermediate/features.json\")) as f:\n",
    "    all_features = json.load(f)\n",
    "\n",
    "# Load threshold\n",
    "with open(os.path.join(BASE_PATH, \"Models/threshold.json\")) as f:\n",
    "    threshold = json.load(f)[\"threshold\"]\n",
    "\n",
    "# Load LSTM autoencoder\n",
    "model = load_model(os.path.join(BASE_PATH, \"Models/lstm_autoencoder.h5\"), compile=False)\n",
    "print(\"LSTM autoencoder loaded âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NznbIA2XZeWw"
   },
   "source": [
    "# Start Spark Session with Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17718,
     "status": "ok",
     "timestamp": 1763073697302,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "iS7nf-HmWbr0"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"IoMT_Anomaly_Detector\")\n",
    "         .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\")\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark session started âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQqW3pspZiAr"
   },
   "source": [
    "# Define schema for incoming JSON from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1763073859561,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "0xOYznV-WeAl"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"flow_duration\", FloatType()),\n",
    "    StructField(\"Header_Length\", FloatType()),\n",
    "    StructField(\"Protocol Type\", FloatType()),\n",
    "    StructField(\"Duration\", FloatType()),\n",
    "    StructField(\"Rate\", FloatType()),\n",
    "    StructField(\"Srate\", FloatType()),\n",
    "    StructField(\"Drate\", FloatType()),\n",
    "    StructField(\"fin_flag_number\", FloatType()),\n",
    "    StructField(\"syn_flag_number\", FloatType()),\n",
    "    StructField(\"rst_flag_number\", FloatType()),\n",
    "    StructField(\"psh_flag_number\", FloatType()),\n",
    "    StructField(\"ack_flag_number\", FloatType()),\n",
    "    StructField(\"ece_flag_number\", FloatType()),\n",
    "    StructField(\"cwr_flag_number\", FloatType()),\n",
    "    StructField(\"ack_count\", FloatType()),\n",
    "    StructField(\"syn_count\", FloatType()),\n",
    "    StructField(\"fin_count\", FloatType()),\n",
    "    StructField(\"urg_count\", FloatType()),\n",
    "    StructField(\"rst_count\", FloatType()),\n",
    "    StructField(\"HTTP\", FloatType()),\n",
    "    StructField(\"HTTPS\", FloatType()),\n",
    "    StructField(\"DNS\", FloatType()),\n",
    "    StructField(\"Telnet\", FloatType()),\n",
    "    StructField(\"SMTP\", FloatType()),\n",
    "    StructField(\"SSH\", FloatType()),\n",
    "    StructField(\"IRC\", FloatType()),\n",
    "    StructField(\"TCP\", FloatType()),\n",
    "    StructField(\"UDP\", FloatType()),\n",
    "    StructField(\"DHCP\", FloatType()),\n",
    "    StructField(\"ARP\", FloatType()),\n",
    "    StructField(\"ICMP\", FloatType()),\n",
    "    StructField(\"IPv\", FloatType()),\n",
    "    StructField(\"LLC\", FloatType()),\n",
    "    StructField(\"Tot sum\", FloatType()),\n",
    "    StructField(\"Min\", FloatType()),\n",
    "    StructField(\"Max\", FloatType()),\n",
    "    StructField(\"AVG\", FloatType()),\n",
    "    StructField(\"Std\", FloatType()),\n",
    "    StructField(\"Tot size\", FloatType()),\n",
    "    StructField(\"IAT\", FloatType()),\n",
    "    StructField(\"Number\", FloatType()),\n",
    "    StructField(\"Magnitue\", FloatType()),\n",
    "    StructField(\"Radius\", FloatType()),\n",
    "    StructField(\"Covariance\", FloatType()),\n",
    "    StructField(\"Variance\", FloatType()),\n",
    "    StructField(\"Weight\", FloatType()),\n",
    "    StructField(\"label\", StringType())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqDTTca5ZmGB"
   },
   "source": [
    "# Subscribe to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4201,
     "status": "ok",
     "timestamp": 1763073867597,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "iOzRQ4diWgPn"
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "      .option(\"subscribe\", \"iomt_traffic_stream\")\n",
    "      .load())\n",
    "\n",
    "stream_df = df.selectExpr(\"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpOIiSbKZpTU"
   },
   "source": [
    "# Parse JSON to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1763073872100,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "fnDqooVjWisr"
   },
   "outputs": [],
   "source": [
    "json_df = stream_df.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O080DL7cZtBZ"
   },
   "source": [
    "# Convert incoming row â†’ Numpy â†’ scale â†’ select â†’ run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1763073874869,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "1hj2pex5Wnzw"
   },
   "outputs": [],
   "source": [
    "def infer(*cols):\n",
    "    x = np.array(cols, dtype=np.float32).reshape(1, -1)\n",
    "    x_scaled = scaler.transform(x)\n",
    "    idx = [all_features.index(f) for f in selected_features]\n",
    "    x_sel = x_scaled[:, idx]\n",
    "    x_lstm = np.expand_dims(x_sel, axis=1)\n",
    "    recon = model.predict(x_lstm, verbose=0)[0, 0]\n",
    "    err = float(np.mean((x_sel - recon)**2))\n",
    "    anomaly = 1 if err > threshold else 0\n",
    "    return anomaly, err\n",
    "\n",
    "infer_udf = udf(infer, returnType=StructType([\n",
    "    StructField(\"anomaly\", StringType()),\n",
    "    StructField(\"error\", FloatType())\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG9F8QHjZyYw"
   },
   "source": [
    "# Apply the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1763073878458,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "qWqUl7_BWrPC"
   },
   "outputs": [],
   "source": [
    "feature_columns = [col(f).cast(\"float\") for f in all_features]  # only numeric\n",
    "result_df = json_df.withColumn(\"prediction\", infer_udf(*feature_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AqoXdtEZ1dv"
   },
   "source": [
    "# Output anomalies to console / DB / dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1763073881511,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "6QC0WXQEWtyQ"
   },
   "outputs": [],
   "source": [
    "query = (result_df\n",
    "          .select(\"flow_duration\", \"prediction.*\")\n",
    "          .writeStream\n",
    "          .outputMode(\"append\")\n",
    "          .format(\"console\")\n",
    "          .option(\"truncate\", False)\n",
    "          .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 181627,
     "status": "error",
     "timestamp": 1763074065604,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "tBF_3CkFWxvB",
    "outputId": "dffa1ea8-8be6-419f-d76d-47e98ddbeaf2"
   },
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 109a8ea6-dcf2-458a-b498-af5f81b333a3, runId = a3ee6ab7-36af-44f3-b734-e7244aa4996d] terminated with exception: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeTopics",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4070487827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 109a8ea6-dcf2-458a-b498-af5f81b333a3, runId = a3ee6ab7-36af-44f3-b734-e7244aa4996d] terminated with exception: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeTopics"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMosymtRoidJwfx6KirMFuS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
