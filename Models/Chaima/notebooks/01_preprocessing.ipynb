{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16959,
     "status": "ok",
     "timestamp": 1763835623870,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "4_4-A_d1G3KL",
    "outputId": "768e36b7-be05-4b09-93f6-1f5e919d1c68"
   },
   "outputs": [],
   "source": "# --- First cell for all notebooks: ---\n# works locally and in Google Colab \n\nimport sys\nimport os\n\n\n# Detect if running in Google Colab\ntry:\n    import google.colab\n    IN_COLAB = True\nexcept ImportError:\n    IN_COLAB = False\n\nif IN_COLAB:\n    # Mount Google Drive in Colab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # Set BASE_PATH assuming repo is cloned in Colab\n    BASE_PATH = \"/content/Real-Time-Anomaly-Detection-in-IoMT-AD-Project\"\nelse:\n    # Local environment: BASE_PATH = project root (three levels up from Models/Chaima/notebooks)\n    BASE_PATH = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n\nprint(f\"üìÅ BASE_PATH detected as: {BASE_PATH}\")\n\n# Add Models/Chaima folder to Python path for imports\nsys.path.append(os.path.join(BASE_PATH, \"Models\", \"Chaima\"))\nprint(\"‚úÖ PYTHONPATH updated. Models/Chaima folder included.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77728,
     "status": "ok",
     "timestamp": 1763071564581,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "l3_8mHssHgpa",
    "outputId": "81bd14eb-aab4-4aed-955d-e7c2dcff6627"
   },
   "outputs": [],
   "source": "from dataset_loader import load_splits\nfrom preprocess import preprocess\nfrom feature_selection import feature_selection\nimport json\n\n# Load raw dataset splits\ntrain, val, test = load_splits(BASE_PATH)\n\n# Preprocess\nX_train_s, X_val_s, X_test_s, features = preprocess(train, val, test, BASE_PATH)\n\n# Feature selection\nselected_features = feature_selection(X_train_s, features, BASE_PATH)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCiZTSpoO1ci"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "INTERMEDIATE_DIR = os.path.join(BASE_PATH, \"Dataset/intermediate\")\n",
    "os.makedirs(INTERMEDIATE_DIR, exist_ok=True)\n",
    "\n",
    "# Save preprocessed arrays\n",
    "np.save(os.path.join(INTERMEDIATE_DIR, \"X_train_s.npy\"), X_train_s)\n",
    "np.save(os.path.join(INTERMEDIATE_DIR, \"X_val_s.npy\"), X_val_s)\n",
    "np.save(os.path.join(INTERMEDIATE_DIR, \"X_test_s.npy\"), X_test_s)\n",
    "\n",
    "# Save all feature names\n",
    "with open(os.path.join(INTERMEDIATE_DIR, \"features.json\"), \"w\") as f:\n",
    "    json.dump(features, f, indent=4)\n",
    "\n",
    "# Load selected features (for display)\n",
    "with open(os.path.join(BASE_PATH, \"Spark/selected_features.json\")) as f:\n",
    "    sel = json.load(f)\n",
    "\n",
    "print(f\"Number of selected features: {len(sel)}\")\n",
    "print(f\"First 10 features: {sel[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1763071578115,
     "user": {
      "displayName": "Chaima Mhemed",
      "userId": "13700094349980969421"
     },
     "user_tz": -60
    },
    "id": "XZJqMlOHMcIb",
    "outputId": "1ee54e4f-4329-4e79-99a0-814aa652ffb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,\n",
       " ['flow_duration',\n",
       "  'Header_Length',\n",
       "  'Protocol Type',\n",
       "  'Duration',\n",
       "  'Rate',\n",
       "  'fin_flag_number',\n",
       "  'syn_flag_number',\n",
       "  'rst_flag_number',\n",
       "  'psh_flag_number',\n",
       "  'ack_flag_number'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(BASE_PATH, \"Spark/selected_features.json\")) as f:\n",
    "    sel = json.load(f)\n",
    "\n",
    "len(sel), sel[:10]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtT3XbTr5bWtlePvFxjqa3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}